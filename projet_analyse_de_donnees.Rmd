---
title: "Projet - Breast Cancer Analysis"
author: "Nicolas SALVAN - Alexandre CORRIOU"
date: "2024-05-17"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Lecture des données

## Importation du dataset 

```{r}
data <- read.csv("data/breast-cancer.csv", header = TRUE, sep = ",")
```


## Affichage des premières lignes

```{r}
head(data)
```

## Affichage des dimensions

```{r}
dim(data)
```
## Affichage des types de données

```{r}
str(data)
```


## Conversion des données qualitatives en factor

```{r}
data$diagnosis <- as.factor(data$diagnosis)
str(data)
```




## Nettoyage 

```{r}
# suppression des NaNs
data <- na.omit(data)

# suppression des colonnes inutiles : identifiant de la patiente 
data <- data[,-c(1)]
head(data)
```



# Statistiques descriptives

## Résumé des données

```{r}
summary(data)
```

## Distribution des données


### Distribution des diagnostics

```{r}
table(data$diagnosis)
```


```{r}
barplot(table(data$diagnosis))
```

```{r}
prop.table(table(data$diagnosis))
```



### Distribution des variables

```{r}
par(mfrow = c(3,3))
for(i in 2:31){
  hist(data[,i], main = colnames(data)[i], xlab = colnames(data)[i], col = "blue")
}
```

## Matrice de corrélation

```{r}
correlation <- cor(data[2:31])
```


La matrice est illisible, nous devons installer un package pour la rendre plus lisible.
```{r}
# install.packages("corrplot")
library(corrplot)
```

```{r}
corrplot(correlation, method = "ellipse", type = "upper", order = "hclust", tl.col = "black", tl.srt = 90, tl.cex = 0.7)

```


On remarque que les variables sont très corrélées entre elles. Il faudra faire attention à la multicollinéarité lors de la modélisation.    
 
On affiche les variables avec un coefficient de corrélation supérieur à 0.98.

```{r}
# Fonction carrément volée sur internet https://rpubs.com/sediaz/Correlations 
corr_check <- function(Dataset, threshold){
  matriz_cor <- cor(Dataset)
  matriz_cor

  for (i in 1:nrow(matriz_cor)){
    correlations <-  which((abs(matriz_cor[i,i:ncol(matriz_cor)]) > threshold) & (matriz_cor[i,i:ncol(matriz_cor)] != 1))
  
    if(length(correlations)> 0){
      lapply(correlations,FUN =  function(x) (cat(paste(colnames(Dataset)[i], "with",colnames(Dataset)[x]), "\n")))
     
    }
  }
}
```

```{r}
corr_check(data[2:31], 0.98)
```
On remarque que les colonnes liées sont le rayon, le périmètre, l'aire. On va supprimer le périmètre car, de part la forme circulaire des cancers, il peut être calculé comme étant 2 * pi * rayon. 

```{r}
#data <- data[,-c(4, 14, 24)]
# head(data)
```


```{r}
#correlation <- cor(data[2:28])
# check_corr <- corr_check(data[2:28], 0.98)
```
On supprime area_mean car elle est corrélée avec radius_mean et radius_worst. (Idée nuuuuuuullleeeeee) 


```{r}
# data <- data[,-c(4)]
# head(data)
```

# Modélisation

## Séparation des données

```{r}
split_data <- function (data, train_ratio) {
  
  set.seed(1)
  n <- nrow(data)
  p <- ncol(data)-1
  test.ratio <- 1 - train_ratio
  n.test <- round(n*test.ratio)
  n.test

  
  
  set.seed(123)
  train_index <- sample(1:nrow(data), n.test)
  train_data <- data[-train_index,]
  test_data <- data[train_index,]
  return(list(train_data = train_data, test_data = test_data))
}
```

```{r}
data_split <- split_data(data, 0.8)
train_data <- data_split$train_data
test_data <- data_split$test_data

```

```{r}

```



## AFD 

```{r}
# install.packages("FactoMineR")
library("FactoMineR")
```

```{r}
res.pca <- PCA(data[2:27], scale.unit = TRUE, graph = FALSE)
```



```{r}
# Affichage des variables ayant le plus d'influence sur les axes
plot(res.pca, choix = "var", habillage = 1, cex = 0.8, col.var = "black", select = "contrib 8")
```




```{r}
# AFD 
res.afd <- PCA(data, quali.sup = 1, scale.unit = TRUE, graph = FALSE)
plot(res.afd, choix = "ind", habillage = 1, cex = 0.8, col.var = "black", select = "contrib 8")
# Affichage de l'axe discriminant

```
```{r}
# cercle des corrélation AFD
plot(res.afd, choix = "var", cex = 0.8, col.var = "black", select = "contrib 8")
```

```{r}
```


## Modèles probabilistes

```{r}
# install.packages("MASS")
library(MASS)
```


```{r}
afd_lda_model <- lda(diagnosis ~ ., data = train_data)
```

```{r}
pred_afd <- predict(afd_lda_model, test_data, prior=c(0.5, 0.5))
#length(pred_afd$class)
#length(data$diagnosis)
table(pred_afd$class, test_data$diagnosis)
```

### LDA 




```{r}
pred_lda <- predict(afd_lda_model, test_data)
table(pred_lda$class, test_data$diagnosis)
```



### QDA 

```{r}
qda_model <- qda(diagnosis ~ ., data = train_data)
```

```{r}
pred_qda <- predict(qda_model, test_data)
table(pred_qda$class, test_data$diagnosis)
```

## Modèle semi-paramétrique: Régression logistique

```{r}
glm_model <- glm(diagnosis ~ ., data = train_data, family = binomial)
```

```{r}
pred_glm <- predict(glm_model, test_data, type = "response")
pred_glm_qual <- ifelse(pred_glm > 0.5, "M", "B")
table(pred_glm_qual, test_data$diagnosis)
```
## Modèle non-paramétrique

### Random Forest

```{r}
# install.packages("randomForest")*
library("randomForest")
```

```{r}
rf_model <- randomForest(train_data$diagnosis ~ ., data = train_data, ntree = 100)
```

```{r}
pred_rf <- predict(rf_model, test_data, type="prob")
pred_rf_fact <- ifelse(pred_rf[,2] > 0.5, "M", "B")
prob_rf <- pred_rf[, "B"]
table(pred_rf_fact, test_data$diagnosis)
```


### SVM

On l'a pas fait en cours mais en projet industriel (: 

```{r}
# install.packages("e1071")
library(e1071)
```

```{r}
svm_model <- svm(diagnosis ~ ., data = train_data, kernel = "linear",  probability = TRUE )
```

```{r}
pred_svm <- predict(svm_model, test_data, probability = TRUE)
pred_svm_prob <- attr(pred_svm, "probabilities")
table(pred_svm, test_data$diagnosis)
```

# Evaluation des modèles

## Courbe ROC

```{r}
# install.packages("pROC")
library("pROC")
```

```{r}
roc_lda <- roc(test_data$diagnosis, pred_lda$posterior[,2])
roc_qda <- roc(test_data$diagnosis, pred_qda$posterior[,2])
roc_glm <- roc(test_data$diagnosis, pred_glm)
roc_rf <- roc(test_data$diagnosis, prob_rf)
roc_svm <- roc(test_data$diagnosis, pred_svm_prob[,2])

```

```{r}
plot(roc_lda, col = "red", main = "Courbe ROC")
plot(roc_qda, col = "blue", add = TRUE)
plot(roc_glm, col = "green", add = TRUE)
plot(roc_rf, col = "purple", add = TRUE)
plot(roc_svm, col = "orange", add = TRUE)
legend("bottomright", legend = c("LDA", "QDA", "GLM", "RF", "SVM"), col = c("red", "blue", "green", "purple", "orange"), lty = 1)

```

```{r}
df.auc <- data.frame(model = c("LDA", "QDA", "GLM", "RF", "SVM"), auc = c(auc(roc_lda), auc(roc_qda), auc(roc_glm), auc(roc_rf), auc(roc_svm)))
df.auc
```


