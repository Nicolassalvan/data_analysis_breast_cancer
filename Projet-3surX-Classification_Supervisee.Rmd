---
title: "Projet - 3surX - Classification Supervisée"
author: "Nicolas SALVAN - Alexandre CORRIOU"
date: "2024-05-23"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Ce document contient le code pour *modéliser les données*. Nous allons réaliser de l'apprentissage supervisé pour prédire le diagnostic des patientes. 

# Lecture des données nettoyées 

## Importation du dataset 

```{r}
data <- read.csv("data/data_cleaned.csv", header = TRUE, sep = ",")
data$diagnosis <- as.factor(data$diagnosis)

train_data <- read.csv("data/train_data.csv", header = TRUE, sep = ",")
train_data$diagnosis <- as.factor(train_data$diagnosis)

test_data <- read.csv("data/test_data.csv", header = TRUE, sep = ",")
test_data$diagnosis <- as.factor(test_data$diagnosis)
```

## Aperçu rapide 

```{r}
head(data)
# dim(data)
# str(data)
```

# Modélisation 

## AFD (Analyse Factorielle Discriminante)

Nous allons réaliser une AFD pour prédire le diagnostic des patientes. 

```{r}
# install.packages("MASS")
library(MASS)
```

### Lancement du modèle 

```{r}
afd_lda_model <- lda(diagnosis ~ ., data = train_data)
```

### Prédiction 

On prédit les données avec le modèle, en précisant les probabilités a priori. On obtient alors la table de confusion suivante. 

```{r}
pred_afd <- predict(afd_lda_model, test_data, prior=c(0.5, 0.5))
table(pred_afd$class, test_data$diagnosis)
```
On observe que le modèle a prédit 0 faux négatifs et 9 faut positifs. Peut-être que les données d'entrainement ne sont pas assez représentatives, et que l'on a des tailles de classes différentes.

```{r}
table(train_data$diagnosis)
table(test_data$diagnosis)

```
On est en effet sur du 2/3 vs 1/3. Il faudrait réequilibrer les données pour obtenir des résultats plus fiables. 


### Performance du modèle 

```{r}
afd_accuracy <- mean(pred_afd$class == test_data$diagnosis) # accuracy
afd_accuracy
```

On obtient une accuracy de 0.92, ce qui est plutôt bon. 

## LDA (Analyse Discriminante Linéaire)

### Lancement du modèle 

```{r}
# Il a déjà été lancé dans la partie AFD
# afd_lda_model <- lda(diagnosis ~ ., data = train_data)
```

### Prédiction

```{r}
pred_lda <- predict(afd_lda_model, test_data)
table(pred_lda$class, test_data$diagnosis)
```

### Performance du modèle 

```{r}
lda_accuracy <- mean(pred_lda$class == test_data$diagnosis) # accuracy
lda_accuracy
```
On obtient une accuracy un peu plus faible que l'AFD. Cela s'explique par le fait que l'AFD est plus adaptée aux données.

### Simplication du modèle 

On peut simplifier le modèle en ne prenant que les variables les plus importantes. 

```{r}
library(klaR)
```

```{r}
stepwise_lda <- stepclass(diagnosis~., data=train_data, method="lda", direction="backward", output = FALSE)
```

```{r}
summary(stepwise_lda$model$name)
```
On a pu supprimer cinq variables inutiles.
On relance une lda sur le modèle simplifié. 

```{r}
lda_simple_model <- lda(stepwise_lda$formula, data=train_data)
```

```{r}
pred_lda_simple <- predict(lda_simple_model, test_data)
table(pred_lda_simple$class, test_data$diagnosis)
```

```{r}
lda_simple_accuracy <- mean(pred_lda_simple$class == test_data$diagnosis) # accuracy
lda_simple_accuracy
```
On obtient un score meilleur avec le modèle plus léger. Nous avons peut être fait du sur-apprentissage. 


## QDA (Analyse Discriminante Quadratique)

### Lancement du modèle 

```{r}
afd_qda_model <- qda(diagnosis ~ ., data = train_data)
```

### Prédiction

```{r}
pred_qda <- predict(afd_qda_model, test_data)
table(pred_qda$class, test_data$diagnosis)
```
### Performance du modèle 

```{r}
qda_accuracy <- mean(pred_qda$class == test_data$diagnosis) # accuracy
qda_accuracy
```
On obtient une accuracy de 0.95, ce qui est plutôt bon. 

### Simplification du modèle

On peut simplifier le modèle en ne prenant que les variables les plus importantes. 

```{r}
stepwise_qda <- stepclass(diagnosis~., data=train_data, method="qda", direction="backward", output = FALSE)
```

```{r}
qda_simple_model <- qda(stepwise_qda$formula, data=train_data)
```

```{r}
summary(stepwise_qda$model$name)
```
On a pu supprimer deux variables inutiles. On relance une qda sur le modèle simplifié.

```{r}
pred_qda_simple <- predict(qda_simple_model, test_data)
table(pred_qda_simple$class, test_data$diagnosis)
```

```{r}
qda_simple_accuracy <- mean(pred_qda_simple$class == test_data$diagnosis) # accuracy
qda_simple_accuracy

```


## CART

```{r}
library(rpart.plot)
library(rpart)
```

### Lancement du modèle

```{r}
tree <- rpart(diagnosis~., train_data)
```


```{r}
print(tree)
```


### Visualisation de l'arbre


```{r}
rpart.plot(tree, type=4, digits=3,roundint=FALSE)
```

```{r}
tree2 <- rpart(diagnosis~.,train_data,control=rpart.control(minsplit=5))
rpart.plot(tree2, type=4, digits=3)
```


### Elagage des arbres

Nous allons élaguer l'arbre pour éviter le sur-apprentissage.

```{r}
tree_elag <- rpart(diagnosis~.,train_data,control=rpart.control(minsplit=5,cp=0))
rpart.plot(tree_elag, type=4)
```

```{r}
plotcp(tree_elag)
```
On observe une erreur minimale pour arbre de taille 4 ou 7. 

```{r}
cp.opt <- tree_elag$cptable[which.min(tree_elag$cptable[, "xerror"]), "CP"]
tree.opt <- prune(tree_elag,cp=cp.opt)
rpart.plot(tree.opt, type=4)
```

### Prediction

```{r}
pred_cart_opti <- predict(tree.opt, newdata=test_data, type="prob")
pred_cart_opti_qual <- ifelse(pred_cart_opti[,2] > 0.5, "M", "B")
table(pred_cart_opti_qual, test_data$diagnosis)

```

### Performance du modèle

```{r}
cart_accuracy <- mean(pred_cart_opti_qual == test_data$diagnosis) # accuracy
cart_accuracy
```
Pour le modèle CART optimal, on obtient une accuracy de 0.90, ce qui est plutôt bon.

## Random Forest

### Lancement du modèle 

```{r}
# install.packages("randomForest")
library(randomForest)
```

```{r}
rf_model <- randomForest(train_data$diagnosis ~ ., data = train_data, ntree = 100)
```

### Prédiction

```{r}
pred_rf <- predict(rf_model, test_data, type="prob")
pred_rf_fact <- ifelse(pred_rf[,2] > 0.5, "M", "B")
prob_rf <- pred_rf[, "B"]
table(pred_rf_fact, test_data$diagnosis)
```
### Performance du modèle 

```{r}
rf_accuracy <- mean(pred_rf_fact == test_data$diagnosis) # accuracy
rf_accuracy
```

### Erreur OOB (Out-of-Bag)

```{r}
plot(rf_model)
legend("topright", colnames(rf_model$err.rate), col=1:3, lty=1:3)
```
Le taux d'erreur semble s'être stabilisé. On observe que l'erreur OOB pour la classification "M" semble plus élevée que pour la classification "B". Il y a donc un déséquilibre dans les classes. Essayons d'équilibrer le jeu d'entrainement.
 
### Modèle avec données équilibrées


```{r}
train_data_balanced <- read.csv("data/train_data_balanced.csv", header = TRUE, sep = ",")
train_data_balanced$diagnosis <- as.factor(train_data_balanced$diagnosis)
```


```{r}
rf_model_balanced <- randomForest(train_data_balanced$diagnosis ~ ., data = train_data_balanced, ntree = 100)
```

```{r}
plot(rf_model_balanced)
```
Le taux d'erreur semble s'être stabilisé. 

```{r}
pred_rf_balanced <- predict(rf_model_balanced, test_data, type="prob")
pred_rf_fact_balanced <- ifelse(pred_rf_balanced[,2] > 0.5, "M", "B")
prob_rf_balanced <- pred_rf_balanced[, "B"]
table(pred_rf_fact_balanced, test_data$diagnosis)
```

```{r}
rf_accuracy_balanced <- mean(pred_rf_fact_balanced == test_data$diagnosis) # accuracy
rf_accuracy_balanced
```

On obtient des performances similaires. 

## Régression Logistique

### Lancement du modèle 

```{r}
glm_model <- glm(diagnosis ~ ., data = train_data, family = binomial,control = glm.control(maxit = 50)) # Augmentation du nombre d'itérations car le modèle ne converge pas
```

Il y a des warnings, ce qui signifie que le modèle n'est pas bien calibré. 

```{r}
summary(glm_model)
```
### Calcul des odds ratios (OR)

```{r}
exp(coef(glm_model))
```

On observe des extrêmes pour les odds ratios. Cela est dû à la présence de variables corrélées, nous allons donc plus tard simplifier le modèle. 

### Intérêt des variables explicatives

```{r}
res0 =glm(diagnosis ~ 1, family = "binomial", data=train_data)
anova(res0,glm_model,test="Chisq")
```
On observe que l'on a une p-value proche de 0, donc on rejette l'hypothèse nulle et on peut conclure qu'il y a au moins une variables explicative qui est significative.

### Prédiction

```{r}
pred_glm <- predict(glm_model, test_data, type = "response")
pred_glm_qual <- ifelse(pred_glm > 0.5, "M", "B")
table(pred_glm_qual, test_data$diagnosis)
```
### Performance du modèle 

```{r}
glm_accuracy <- mean(pred_glm_qual == test_data$diagnosis) # accuracy
glm_accuracy
```
On obtient une accuracy de 88.6%, ce qui est plutôt bon mais pas aussi bon que les autres modèles.

### Simplification du modèle avec des régressions logistiques pénalisées

Nous allons simplifier le modèle de régression logistique pour supprimer les variables inutiles.

```{r}
# install.packages("glmnet")
library(glmnet)
```
Nous allons réaliser une régression de type Ridge et de type Lasso 


#### Lancement des algorithmes 

```{r}
ridge_model <- glmnet(as.matrix(train_data[, -1]), train_data$diagnosis, alpha = 0, family = "binomial")
lasso_model <- glmnet(as.matrix(train_data[, -1]), train_data$diagnosis, alpha = 1, family = "binomial")
```

L'algorithme a bien convergé. 

#### Visualisation des chemins de régularisation des estimateurs `ridge` et `lasso`

```{r}
plot(ridge_model, label = TRUE)
```

```{r}
plot(ridge_model, xvar = "lambda", label = TRUE)
```

```{r}
plot(lasso_model, label = TRUE)
```

```{r}
plot(lasso_model, xvar = "lambda", label = TRUE)
```

#### Prédictions et performances 

```{r}
pred_ridge <- predict(ridge_model, s = 0.01, newx = as.matrix(test_data[, -1]), type = "response")
pred_ridge_qual <- ifelse(pred_ridge > 0.5, "M", "B")
table(pred_ridge_qual, test_data$diagnosis)
```

```{r}
ridge_accuracy <- mean(pred_ridge_qual == test_data$diagnosis) # accuracy
ridge_accuracy
```

```{r}
pred_lasso <- predict(lasso_model, s = 0.01, newx = as.matrix(test_data[, -1]), type = "response")
pred_lasso_qual <- ifelse(pred_lasso > 0.5, "M", "B")
table(pred_lasso_qual, test_data$diagnosis)
```

```{r}
lasso_accuracy <- mean(pred_lasso_qual == test_data$diagnosis) # accuracy
lasso_accuracy

```
#### Nombre de variables sélectionnées

```{r}
sum(coef(lasso_model, s=exp(-6))!=0)
```

```{r}
sum(coef(ridge_model, s=exp(-6))!=0)
```
On a sélectionné beaucoup moins de variables pour le modèle Lasso, qui a des performances similaires. 








## SVM

Nous avons décidé d'expérimenter un modèle SVM pour voir si les performances sont meilleures, comme nous l'avons fait lors de nos projets industriels. 

### Lancement du modèle 

```{r}
# install.packages("e1071")
library(e1071)
```

```{r}
svm_lin_model <- svm(diagnosis ~ ., data = train_data, kernel = "linear",  probability = TRUE )
svm_rbf_model <- svm(diagnosis ~ ., data = train_data, kernel = "radial",  probability = TRUE )
```

### Prédiction

```{r}
pred_svm_lin <- predict(svm_lin_model, test_data, probability = TRUE)
pred_svm_lin_prob <- attr(pred_svm_lin, "probabilities")
table(pred_svm_lin, test_data$diagnosis)
```

```{r}
pred_svm_rbf <- predict(svm_rbf_model, test_data, probability = TRUE)
pred_svm_rbf_prob <- attr(pred_svm_rbf, "probabilities")
table(pred_svm_rbf, test_data$diagnosis)
```

### Performance du modèle

```{r}
svm_lin_accuracy <- mean(pred_svm_lin == test_data$diagnosis) # accuracy
svm_lin_accuracy
```

```{r}
svm_rbf_accuracy <- mean(pred_svm_rbf == test_data$diagnosis) # accuracy
svm_rbf_accuracy
```
# Evaluation des modèles

## Courbes ROC

```{r}
# install.packages("pROC")
library("pROC")
```

### Calcul des courbes ROC

```{r}
roc_afd <- roc(test_data$diagnosis, pred_afd$posterior[,2])
roc_lda <- roc(test_data$diagnosis, pred_lda$posterior[,2])
roc_lda_simple <- roc(test_data$diagnosis, pred_lda_simple$posterior[,2])
roc_qda <- roc(test_data$diagnosis, pred_qda$posterior[,2])
roc_qda_simple <- roc(test_data$diagnosis, pred_qda_simple$posterior[,2])
# rpc_cart <- roc(test_data$diagnosis, pred_cart)
roc_glm <- roc(test_data$diagnosis, pred_glm)
roc_ridge <- roc(test_data$diagnosis, pred_ridge)
roc_lasso <- roc(test_data$diagnosis, pred_lasso)
roc_rf <- roc(test_data$diagnosis, prob_rf)
roc_rf_balanced <- roc(test_data$diagnosis, prob_rf_balanced)

roc_svm_lin <- roc(test_data$diagnosis, pred_svm_lin_prob[,2])
roc_svm_rbf <- roc(test_data$diagnosis, pred_svm_rbf_prob[,2])
```
```{r}
roc_list <- list(roc_afd, roc_lda, roc_lda_simple, roc_qda, roc_qda_simple, roc_glm, roc_ridge, roc_lasso, roc_rf, roc_rf_balanced, roc_svm_lin, roc_svm_rbf)
legends_list <- c("AFD", "LDA", "LDA simple", "QDA", "QDA simple", "GLM", "Ridge", "Lasso", "RF", "RF balanced", "SVM lin", "SVM rbf")
```

### Affichage des courbes ROC

```{r}
plot(roc_afd, col = "red", main = "Courbes ROC")
cols <- rainbow(length(roc_list))
j <- 1
for (i in roc_list) {
  plot(i, add = TRUE, col = cols[j], label = legends_list[j])
  j <- j + 1
}
legend("bottomright", legend = legends_list, col = cols, lty = 1, cex = 0.8)

```

### AUC

```{r}
df.auc <- data.frame(model = legends_list, auc = sapply(roc_list, function(x) auc(x)))
df.auc
```


```{r}
barplot(df.auc$auc, names.arg = df.auc$model, col = rainbow(length(df.auc$auc)), main = "AUC des modèles", xlab = "Modèles", xlim = c(0,1), cex.names = 0.7, horiz = TRUE, las = 1)
```


```{r}
# AUC sorted 
df.auc_sorted <- df.auc[order(df.auc$auc, decreasing = TRUE),]
barplot(df.auc_sorted$auc, names.arg = df.auc_sorted$model, col = rainbow(length(df.auc_sorted$auc)), main = "AUC des modèles",xlim = c(0,1), xlab = "Modèles", cex.names = 0.7, horiz = TRUE, las = 1)
```

On obtient que le meilleur modèle relativement à l'AUC trouvé est le SVM avec noyau RBF (non-linéaire). Le meilleur modèle trouvé que l'on a vu dans ce cours est le modèle de régression logistique avec pénalisation de Ridge, avec une AUC de 0.981. De plus, le troisième meilleur modèle, à savoir la régression Lasso, a une AUC assez proche de celle de la régression Ridge, tout en dépendant de moins de variables. 

### Comparaison de l'accuarcy des modèles

```{r}
df_accuracy <- data.frame(model = c("AFD", "LDA", "LDA simple", "QDA", "QDA simple", "GLM", "Ridge", "Lasso", "RF", "RF balanced", "SVM lin", "SVM rbf"), accuracy = c(afd_accuracy, lda_accuracy, lda_simple_accuracy, qda_accuracy, qda_simple_accuracy, glm_accuracy, ridge_accuracy, lasso_accuracy, rf_accuracy, rf_accuracy_balanced, svm_lin_accuracy, svm_rbf_accuracy))
```

```{r}
df_accuracy
```


```{r}
barplot(df_accuracy$accuracy, names.arg = df_accuracy$model, col = rainbow(length(df_accuracy$accuracy)), main = "Accuracy des modèles", xlab = "Modèles", cex.names = 0.7, horiz = TRUE, las = 1)
```

```{r}
df_accuracy_sorted <- df_accuracy[order(df_accuracy$accuracy, decreasing = TRUE),]
barplot(df_accuracy_sorted$accuracy, names.arg = df_accuracy_sorted$model, col = rainbow(length(df_accuracy_sorted$accuracy)), main = "Accuracy des modèles", xlab = "Modèles", cex.names = 0.7, horiz = TRUE, las = 1)
```

Les modèles avec les meilleures accuracy sont les modèles de QDA, de régression logistique avec pénalisation de Ridge et de régression Lasso. Comme la QDA n'est pas dans les meilleurs modèles en terme d'AUC, on peut dire que les modèles de régression logistique avec pénalisation de Ridge et de régression Lasso sont les meilleurs modèles de statistiques prédictives que l'on a trouvé.

Le modèle qui a le moins fonctionné est le modèle de régression logistique sans pénalisation. En effet, lorsque nous lancions l'analyse, nous obtenions des warnings ce qui signifiait que le modèle n'était pas bien calibré. 

### Interprétation des résultats

Les modèles de régression logistique avec pénalisation de Ridge et de régression Lasso sont les meilleurs modèles de statistiques prédictives que l'on a trouvé. En effet, ils ont les meilleures AUC et les meilleures accuracy. De plus, le modèle de régression Lasso dépend de moins de variables que le modèle de régression Ridge, ce qui peut être un avantage. 

Le modèle de SVM avec noyau RBF est le meilleur modèle en terme d'AUC, mais il n'est pas le meilleur en terme d'accuracy.

Le modèle de régression logistique sans pénalisation est le moins bon modèle que l'on a trouvé. En effet, il n'était pas bien calibré. 

# Conclusion

Le data-set est exploitable en machine learning, et nous avons pu obtenir des modèles de statistiques prédictives qui ont de bonnes performances. Les modèles de régression logistique avec pénalisation de Ridge et de régression Lasso sont les meilleurs modèles que l'on a trouvé. Ces modèles pourraient permettre de prédire le diagnostic de patientes atteintes de cancer du sein, dans une certaine mesure.
