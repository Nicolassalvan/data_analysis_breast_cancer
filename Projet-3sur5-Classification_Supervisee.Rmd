---
title: "Projet 3/5 Classification Supervisée"
author: "Nicolas SALVAN - Alexandre CORRIOU"
date: "2024-05-23"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Ce document contient le code pour *modéliser les données*. Nous allons réaliser de l'apprentissage supervisé pour prédire le diagnostic des patientes. 

# Lecture des données nettoyées 

## Importation du dataset 

```{r}
data <- read.csv("data/data_cleaned.csv", header = TRUE, sep = ",")
data$diagnosis <- as.factor(data$diagnosis)

train_data <- read.csv("data/train_data.csv", header = TRUE, sep = ",")
train_data$diagnosis <- as.factor(train_data$diagnosis)

test_data <- read.csv("data/test_data.csv", header = TRUE, sep = ",")
test_data$diagnosis <- as.factor(test_data$diagnosis)
```

## Aperçu rapide 

```{r}
head(data)
# dim(data)
# str(data)
```

# Modélisation 

## AFD (Analyse Factorielle Discriminante)

Nous allons réaliser une AFD pour prédire le diagnostic des patientes. 

```{r}
# install.packages("MASS")
library(MASS)
library(pROC)
```

### Lancement du modèle 

```{r}
afd_lda_model <- lda(diagnosis ~ ., data = train_data)
```

### Prédiction 

On prédit les données avec le modèle, en précisant les probabilités a priori. On obtient alors la table de confusion suivante. 

```{r}
pred_afd <- predict(afd_lda_model, test_data, prior=c(0.5, 0.5))
table(pred_afd$class, test_data$diagnosis)
```
On observe que le modèle a prédit 0 faux négatifs et 9 faux positifs. Peut-être que les données d'entrainement ne sont pas assez représentatives, et que l'on a des tailles de classes différentes.

```{r}
table(train_data$diagnosis)
table(test_data$diagnosis)

```
On est en effet sur du 2/3 vs 1/3. Il faudrait réequilibrer les données pour obtenir des résultats plus fiables. 


### Performance du modèle 

```{r}
afd_accuracy <- mean(pred_afd$class == test_data$diagnosis) # accuracy
afd_accuracy
```

On obtient une accuracy de 0.92, ce qui est plutôt bon. 

```{r}
roc_afd <- roc(test_data$diagnosis, pred_afd$posterior[,2])
plot(roc_afd, col = "red", AUC = TRUE, main = "Courbe ROC AFD")
```

```{r}
auc_afd <- auc(roc_afd)
auc_afd
```

## LDA (Analyse Discriminante Linéaire)

### Lancement du modèle 

```{r}
# Il a déjà été lancé dans la partie AFD
# afd_lda_model <- lda(diagnosis ~ ., data = train_data)
```

### Prédiction

```{r}
pred_lda <- predict(afd_lda_model, test_data)
table(pred_lda$class, test_data$diagnosis)
```

### Performance du modèle 

```{r}
lda_accuracy <- mean(pred_lda$class == test_data$diagnosis) # accuracy
lda_accuracy
```
On obtient une accuracy un peu plus faible que l'AFD. Cela s'explique par le fait que l'AFD est plus adaptée aux données.

```{r}
roc_lda <- roc(test_data$diagnosis, pred_lda$posterior[,2])
plot(roc_lda, col = "red", AUC = TRUE, main = "Courbe ROC LDA")
```

```{r}
auc_lda <- auc(roc_lda)
auc_lda
```

### Simplication du modèle 

On peut simplifier le modèle en ne prenant que les variables les plus importantes. 

```{r}
library(klaR)
```

```{r}
stepwise_lda <- stepclass(diagnosis~., data=train_data, method="lda", direction="backward", output = FALSE)
```

```{r}
summary(stepwise_lda$model$name)
```
On a pu supprimer cinq variables inutiles.
On relance une lda sur le modèle simplifié. 

```{r}
lda_simple_model <- lda(stepwise_lda$formula, data=train_data)
```

```{r}
pred_lda_simple <- predict(lda_simple_model, test_data)
table(pred_lda_simple$class, test_data$diagnosis)
```

```{r}
lda_simple_accuracy <- mean(pred_lda_simple$class == test_data$diagnosis) # accuracy
lda_simple_accuracy
```
On obtient un score meilleur avec le modèle plus léger. Nous avons peut être fait du sur-apprentissage. 

```{r}
roc_lda_simple <- roc(test_data$diagnosis, pred_lda_simple$posterior[,2])
plot(roc_lda_simple, col = "red", AUC = TRUE, main = "Courbe ROC LDA simple")
```

```{r}
auc_lda_simple <- auc(roc_lda_simple)
auc_lda_simple
```


## QDA (Analyse Discriminante Quadratique)

### Lancement du modèle 

```{r}
afd_qda_model <- qda(diagnosis ~ ., data = train_data)
```

### Prédiction

```{r}
pred_qda <- predict(afd_qda_model, test_data)
table(pred_qda$class, test_data$diagnosis)
```
### Performance du modèle 

```{r}
qda_accuracy <- mean(pred_qda$class == test_data$diagnosis) # accuracy
qda_accuracy
```
On obtient une accuracy de 0.947, ce qui est plutôt bon. 

```{r}
roc_qda <- roc(test_data$diagnosis, pred_qda$posterior[,2])
plot(roc_qda, col = "red", AUC = TRUE, main = "Courbe ROC QDA")
```

```{r}
auc_qda <- auc(roc_qda)
auc_qda
```


### Simplification du modèle

On peut simplifier le modèle en ne prenant que les variables les plus importantes. 

```{r}
stepwise_qda <- stepclass(diagnosis~., data=train_data, method="qda", direction="backward", output = FALSE)
```

```{r}
qda_simple_model <- qda(stepwise_qda$formula, data=train_data)
```

```{r}
summary(stepwise_qda$model$name)
```
On a pu supprimer deux variables inutiles. On relance une qda sur le modèle simplifié.

```{r}
pred_qda_simple <- predict(qda_simple_model, test_data)
table(pred_qda_simple$class, test_data$diagnosis)
```

```{r}
qda_simple_accuracy <- mean(pred_qda_simple$class == test_data$diagnosis) # accuracy
qda_simple_accuracy

```
```{r}
roc_qda_simple <- roc(test_data$diagnosis, pred_qda_simple$posterior[,2])
plot(roc_qda_simple, col = "red", AUC = TRUE, main = "Courbe ROC QDA simple")
```

```{r}
auc_qda_simple <- auc(roc_qda_simple)
auc_qda_simple
```


## CART

```{r}
library(rpart.plot)
library(rpart)
```

### Lancement du modèle

Nous allons construire un arbre naïf, avec les paramètres par défaut. 

```{r}
tree <- rpart(diagnosis~., train_data)
```


```{r}
print(tree)
```


### Visualisation de l'arbre


```{r}
rpart.plot(tree, type=4, digits=3,roundint=FALSE)
```

### Prédiction et performance du modèle naïf

```{r}
pred_cart_naive <- predict(tree, newdata=test_data, type="prob")
pred_cart_naive_qual <- ifelse(pred_cart_naive[,2] > 0.5, "M", "B")
table(pred_cart_naive_qual, test_data$diagnosis)
```
```{r}
cart_naive_accuracy <- mean(pred_cart_naive_qual == test_data$diagnosis) # accuracy
cart_naive_accuracy
```

```{r}
roc_cart_naive <- roc(test_data$diagnosis, pred_cart_naive[,2], plot=TRUE, col = "red", main = "Courbe ROC CART naïve")
```

```{r}
auc_cart_naive <- auc(roc_cart_naive)
auc_cart_naive
```
On n'obtient pas de très bonnes performances avec le modèle naïf. Nous allons essayer de l'améliorer.


### Arbre simplifié

Nous allons simplifier l'arbre pour éviter le sur-apprentissage.


```{r}
tree2 <- rpart(diagnosis~.,train_data,control=rpart.control(minsplit=5))
rpart.plot(tree2, type=4, digits=3)
```

### Prédiction et performance du modèle simplifié

```{r}
pred_cart <- predict(tree2, newdata=test_data, type="prob")
pred_cart_qual <- ifelse(pred_cart[,2] > 0.5, "M", "B")
table(pred_cart_qual, test_data$diagnosis)
```

```{r}
cart_accuracy <- mean(pred_cart_qual == test_data$diagnosis) # accuracy
cart_accuracy
```
```{r}
roc_cart <- roc(test_data$diagnosis, pred_cart[,2])
plot(roc_cart, col = "red", AUC = TRUE, main = "Courbe ROC CART")
```

```{r}
auc_cart <- auc(roc_cart)
auc_cart
```
Cela n'a pas amélioré le modèle. Nous allons essayer de faire de l'élagage.


### Elagage des arbres

Nous allons élaguer l'arbre pour éviter le sur-apprentissage. 


```{r}
tree_elag <- rpart(diagnosis~.,train_data,control=rpart.control(minsplit=5,cp=0))
rpart.plot(tree_elag, type=4)
```

```{r}
plotcp(tree_elag)
```
On observe une erreur minimale pour arbre de taille entre 4 et 7. 

```{r}
cp.opt <- tree_elag$cptable[which.min(tree_elag$cptable[, "xerror"]), "CP"]
tree.opt <- prune(tree_elag,cp=cp.opt)
rpart.plot(tree.opt, type=4)
```

### Prediction

```{r}
pred_cart_opti <- predict(tree.opt, newdata=test_data, type="prob")
pred_cart_opti_qual <- ifelse(pred_cart_opti[,2] > 0.5, "M", "B")
table(pred_cart_opti_qual, test_data$diagnosis)

```

### Performance du modèle

```{r}
cart_opti_accuracy <- mean(pred_cart_opti_qual == test_data$diagnosis) # accuracy
cart_opti_accuracy
```
Pour le modèle CART optimal, on obtient une accuracy de 0.91, ce qui est meilleur que les version précédentes de CART. 

```{r}
roc_cart_opti <- roc(test_data$diagnosis, pred_cart_opti[,2])
plot(roc_cart_opti, col = "red", AUC = TRUE, main = "Courbe ROC CART optimal")
```

```{r}
auc_cart_opti <- auc(roc_cart_opti)
auc_cart_opti
```
On obtient une AUC de 0.8788, ce qui est moins bon que les modèles précédents.

## Random Forest

### Lancement du modèle 

```{r}
# install.packages("randomForest")
library(randomForest)
```

```{r}
rf_model <- randomForest(train_data$diagnosis ~ ., data = train_data, ntree = 100)
```

### Prédiction

```{r}
pred_rf <- predict(rf_model, test_data, type="prob")
pred_rf_fact <- ifelse(pred_rf[,2] > 0.5, "M", "B")
prob_rf <- pred_rf[, "B"]
table(pred_rf_fact, test_data$diagnosis)
```
### Performance du modèle 

```{r}
rf_accuracy <- mean(pred_rf_fact == test_data$diagnosis) # accuracy
rf_accuracy
```

```{r}
roc_rf <- roc(test_data$diagnosis, prob_rf)
plot(roc_rf, col = "red", AUC = TRUE, main = "Courbe ROC RandomForest")
```

```{r}
auc_rf <- auc(roc_rf)
auc_rf
```


### Erreur OOB (Out-of-Bag)

```{r}
plot(rf_model)
legend("topright", colnames(rf_model$err.rate), col=1:3, lty=1:3)
```
Le taux d'erreur semble s'être stabilisé. On observe que l'erreur OOB pour la classification "M" semble plus élevée que pour la classification "B". Il y a donc un déséquilibre dans les classes. Essayons d'équilibrer le jeu d'entrainement.
 
### Modèle avec données équilibrées


```{r}
train_data_balanced <- read.csv("data/train_data_balanced.csv", header = TRUE, sep = ",")
train_data_balanced$diagnosis <- as.factor(train_data_balanced$diagnosis)
```


```{r}
rf_model_balanced <- randomForest(train_data_balanced$diagnosis ~ ., data = train_data_balanced, ntree = 100)
```

```{r}
plot(rf_model_balanced)
```
Le taux d'erreur semble s'être stabilisé. 

```{r}
pred_rf_balanced <- predict(rf_model_balanced, test_data, type="prob")
pred_rf_fact_balanced <- ifelse(pred_rf_balanced[,2] > 0.5, "M", "B")
prob_rf_balanced <- pred_rf_balanced[, "B"]
table(pred_rf_fact_balanced, test_data$diagnosis)
```

```{r}
rf_accuracy_balanced <- mean(pred_rf_fact_balanced == test_data$diagnosis) # accuracy
rf_accuracy_balanced
```

On obtient des performances similaires. 


```{r}
roc_rf_balanced <- roc(test_data$diagnosis, prob_rf_balanced)
plot(roc_rf_balanced, col = "red", AUC = TRUE, main = "Courbe ROC RandomForest with balanced data")
```

```{r}
auc_rf_balanced <- auc(roc_rf_balanced)
auc_rf_balanced
```


## Régression Logistique

### Lancement du modèle 

```{r}
glm_model <- glm(diagnosis ~ ., data = train_data, family = binomial,control = glm.control(maxit = 50)) # Augmentation du nombre d'itérations car le modèle ne converge pas
```

Il y a des warnings, ce qui signifie que le modèle n'est pas bien calibré. 

```{r}
summary(glm_model)
```
### Calcul des odds ratios (OR)

```{r}
exp(coef(glm_model))
```

On observe des extrêmes pour les odds ratios. Cela est dû à la présence de variables corrélées, nous allons donc plus tard simplifier le modèle. 

### Intérêt des variables explicatives

```{r}
res0 =glm(diagnosis ~ 1, family = "binomial", data=train_data)
anova(res0,glm_model,test="Chisq")
```
On observe que l'on a une p-value proche de 0, donc on rejette l'hypothèse nulle et on peut conclure qu'il y a au moins une variables explicative qui est significative.

### Prédiction

```{r}
pred_glm <- predict(glm_model, test_data, type = "response")
pred_glm_qual <- ifelse(pred_glm > 0.5, "M", "B")
table(pred_glm_qual, test_data$diagnosis)
```
### Performance du modèle 

```{r}
glm_accuracy <- mean(pred_glm_qual == test_data$diagnosis) # accuracy
glm_accuracy
```
On obtient une accuracy de 88.6%, ce qui est plutôt bon mais pas aussi bon que les autres modèles.

```{r}
roc_glm <- roc(test_data$diagnosis, pred_glm)
plot(roc_glm, col = "red", AUC = TRUE, main = "Courbe ROC Régression logstique multiple")
```

```{r}
auc_glm <- auc(roc_glm)
auc_glm
```


### Simplification du modèle avec des régressions logistiques pénalisées


Nous allons simplifier le modèle de régression logistique pour supprimer les variables inutiles. Pour se faire, nous allons réaliser une régression de type Ridge et de type Lasso 


```{r}
# install.packages("glmnet")
library(glmnet)
```


#### Lancement des algorithmes 



```{r}
ridge_model <- glmnet(as.matrix(train_data[, -1]), train_data$diagnosis, alpha = 0, family = "binomial")
lasso_model <- glmnet(as.matrix(train_data[, -1]), train_data$diagnosis, alpha = 1, family = "binomial")
```



L'algorithme a bien convergé. 


#### Visualisation des chemins de régularisation des estimateurs `ridge` et `lasso`



```{r}
plot(ridge_model, label = TRUE)
```

```{r}
plot(ridge_model, xvar = "lambda", label = TRUE)
```

```{r}
plot(lasso_model, label = TRUE)
```

```{r}
plot(lasso_model, xvar = "lambda", label = TRUE)
```

#### Prédictions et performances 

```{r}
pred_ridge <- predict(ridge_model, s = 0.01, newx = as.matrix(test_data[, -1]), type = "response")
pred_ridge_qual <- ifelse(pred_ridge > 0.5, "M", "B")
table(pred_ridge_qual, test_data$diagnosis)
```

```{r}
ridge_accuracy <- mean(pred_ridge_qual == test_data$diagnosis) # accuracy
ridge_accuracy
```

```{r}
pred_lasso <- predict(lasso_model, s = 0.01, newx = as.matrix(test_data[, -1]), type = "response")
pred_lasso_qual <- ifelse(pred_lasso > 0.5, "M", "B")
table(pred_lasso_qual, test_data$diagnosis)
```

```{r}
lasso_accuracy <- mean(pred_lasso_qual == test_data$diagnosis) # accuracy
lasso_accuracy

```

```{r}
roc_ridge <- roc(test_data$diagnosis, pred_ridge)
roc_lasso <- roc(test_data$diagnosis, pred_lasso)
```


```{r}
plot(roc_ridge, col = "red", AUC = TRUE, main = "Courbe ROC Régression Ridge")
```

```{r}
plot(roc_lasso, col = "red", AUC = TRUE, main = "Courbe ROC Régression Lasso")
```

```{r}
auc_ridge <- auc(roc_ridge)
auc_lasso <- auc(roc_lasso)
auc_ridge
auc_lasso

```

#### Nombre de variables sélectionnées

```{r}
sum(coef(lasso_model, s=exp(-6))!=0)
```

```{r}
sum(coef(ridge_model, s=exp(-6))!=0)
```
On a sélectionné beaucoup moins de variables pour le modèle Lasso, qui a des performances similaires. 




## SVM

Nous avons décidé d'expérimenter un modèle SVM pour voir si les performances sont meilleures, comme nous l'avons fait lors de nos projets industriels. 

### Lancement du modèle 

```{r}
# install.packages("e1071")
library(e1071)
```

```{r}
svm_lin_model <- svm(diagnosis ~ ., data = train_data, kernel = "linear",  probability = TRUE )
svm_rbf_model <- svm(diagnosis ~ ., data = train_data, kernel = "radial",  probability = TRUE )
```

### Prédiction

```{r}
pred_svm_lin <- predict(svm_lin_model, test_data, probability = TRUE)
pred_svm_lin_prob <- attr(pred_svm_lin, "probabilities")
table(pred_svm_lin, test_data$diagnosis)
```

```{r}
pred_svm_rbf <- predict(svm_rbf_model, test_data, probability = TRUE)
pred_svm_rbf_prob <- attr(pred_svm_rbf, "probabilities")
table(pred_svm_rbf, test_data$diagnosis)
```

### Performance du modèle

```{r}
svm_lin_accuracy <- mean(pred_svm_lin == test_data$diagnosis) # accuracy
svm_lin_accuracy
```

```{r}
svm_rbf_accuracy <- mean(pred_svm_rbf == test_data$diagnosis) # accuracy
svm_rbf_accuracy
```

```{r}
roc_svm_lin <- roc(test_data$diagnosis, pred_svm_lin_prob[,2])
roc_svm_rbf <- roc(test_data$diagnosis, pred_svm_rbf_prob[,2])
```

```{r}
plot(roc_svm_lin, col = "red", AUC = TRUE, main = "Courbe ROC SVM linéaire")
```

```{r}
plot(roc_svm_rbf, col = "red", AUC = TRUE, main = "Courbe ROC SVM RBF")
```

```{r}
auc_svm_lin <- auc(roc_svm_lin)
auc_svm_rbf <- auc(roc_svm_rbf)
auc_svm_lin
auc_svm_rbf
```
On obtient des AUC très satisfaisantes. 


# Evaluation des modèles

## Courbes ROC

```{r}
# install.packages("pROC")
library("pROC")
```

### Calcul des courbes ROC

```{r}
roc_list <- list(roc_afd, roc_lda, roc_lda_simple, roc_qda, roc_qda_simple, roc_glm, roc_ridge, roc_lasso, roc_rf, roc_rf_balanced, roc_svm_lin, roc_svm_rbf, roc_cart_naive, roc_cart, roc_cart_opti)
```

```{r}
legends_list <- c("AFD", "LDA", "LDA simple", "QDA", "QDA simple", "GLM", "Ridge", "Lasso", "RF", "RF.balanced", "SVM lin", "SVM rbf", "CART naive", "CART", "CART opti")
```




### Affichage des courbes ROC

```{r}
plot(roc_afd, col = "red", main = "Courbes ROC")
cols <- rainbow(length(roc_list))
j <- 1
for (i in roc_list) {
  plot(i, add = TRUE, col = cols[j], label = legends_list[j])
  j <- j + 1
}
legend("bottomright", legend = legends_list, col = cols, lty = 1, cex = 0.7)

```





### AUC

```{r}
df.auc <- data.frame(model = legends_list, auc = sapply(roc_list, function(x) auc(x)))
df.auc
```

```{r}
# Sauvegarde des AUC dans un fichier csv
write.csv(df.auc, "data/auc.csv")
```


```{r}
barplot(df.auc$auc, names.arg = df.auc$model, col = rainbow(length(df.auc$auc)), main = "AUC des modèles", xlab = "Modèles", xlim = c(0,1), cex.names = 0.7, horiz = TRUE, las = 1)
```


```{r}
df.auc_sorted <- df.auc[order(df.auc$auc, decreasing = TRUE),]
barplot(df.auc_sorted$auc, names.arg = df.auc_sorted$model, col = rainbow(length(df.auc_sorted$auc)), main = "AUC des modèles",xlim = c(0,1), xlab = "Modèles", cex.names = 0.7, horiz = TRUE, las = 1)
```

```{r}
boxplot(df.auc$auc, main = "Boxplot des AUC des modèles", ylab = "AUC", col = "cyan")
```
```{r}
summary(df.auc$auc)
```


On obtient que le *meilleur modèle* relativement à l'AUC trouvé est le SVM avec noyau RBF (non-linéaire). Le meilleur modèle trouvé que l'on a vu dans ce cours est le modèle de régression logistique avec pénalisation de Ridge, avec une AUC de 0.981. De plus, le troisième meilleur modèle, à savoir la régression Lasso, a une AUC assez proche de celle de la régression Ridge, tout en dépendant de moins de variables. 

Les *modèles les moins bons* sont les modèles CART, qui ont des AUC inférieures à 0.9, et de régression logistique non pénalisée, qui ont des performances très inférieures aux autres. 


```{r}
# Courbes ROC des 5 meilleurs modèles 

plot(roc_ridge, col = "red", AUC = TRUE, main = "Courbes ROC des 5 meilleurs modèles")
plot(roc_lasso, col = "blue", AUC = TRUE, add = TRUE)
plot(roc_svm_rbf, col = "green", AUC = TRUE, add = TRUE)
plot(roc_rf_balanced, col = "purple", AUC = TRUE, add = TRUE)
plot(roc_afd, col = "orange", AUC = TRUE, add = TRUE)
legend("bottomright", legend = c("Ridge", "Lasso", "SVM RBF", "RF balanced", "AFD"), col = c("red", "blue", "green", "purple", "orange"), lty = 1, cex = 0.7)

```


### Comparaison de l'accuarcy des modèles

```{r}
df_accuracy <- data.frame(model = c("AFD", "LDA", "LDA simple", "QDA", "QDA simple", "GLM", "Ridge", "Lasso", "RF", "RF balanced", "SVM lin", "SVM rbf", "CART naive", "CART", "CART opti"), accuracy = c(afd_accuracy, lda_accuracy, lda_simple_accuracy, qda_accuracy, qda_simple_accuracy, glm_accuracy, ridge_accuracy, lasso_accuracy, rf_accuracy, rf_accuracy_balanced, svm_lin_accuracy, svm_rbf_accuracy, cart_naive_accuracy, cart_accuracy, cart_opti_accuracy))
```


```{r}
df_accuracy
```

```{r}
# Sauvegarde des accuracy dans un fichier csv
write.csv(df_accuracy, "data/accuracy.csv")
```


```{r}
barplot(df_accuracy$accuracy, names.arg = df_accuracy$model, col = rainbow(length(df_accuracy$accuracy)), main = "Accuracy des modèles", xlab = "Modèles", cex.names = 0.7, horiz = TRUE, las = 1)
```

```{r}
df_accuracy_sorted <- df_accuracy[order(df_accuracy$accuracy, decreasing = TRUE),]
barplot(df_accuracy_sorted$accuracy, names.arg = df_accuracy_sorted$model, col = rainbow(length(df_accuracy_sorted$accuracy)), main = "Accuracy des modèles", xlab = "Modèles", cex.names = 0.7, horiz = TRUE, las = 1)
```

```{r}
boxplot(df_accuracy$accuracy, main = "Boxplot des accuracy des modèles", ylab = "Accuracy", col = "blue")
```


```{r}
summary(df_accuracy$accuracy)
```


* Meilleurs Modèles* : Les modèles avec les meilleures accuracy sont les modèles de QDA, de régression logistique avec pénalisation de Ridge et de régression Lasso. Comme la QDA n'est pas dans les meilleurs modèles en terme d'AUC, on peut dire que les modèles de régression logistique avec pénalisation de Ridge et de régression Lasso sont les meilleurs modèles de statistiques prédictives que l'on a trouvé. Une très bonne accuracy pour la QDA peut être un signe de sur-apprentissage.

* Modèles moins efficaces* : Le modèle qui a le moins fonctionné est le modèle de régression logistique sans pénalisation. En effet, lorsque nous lancions l'analyse, nous obtenions des warnings ce qui signifiait que le modèle n'était pas bien calibré. Le modèle CART a également eu des performances moins intéressantes que l'on a pu observer avec son accuracy plus basse que les autres, tout comme le LDA. 

### Interprétation des résultats

Les modèles de régression logistique avec pénalisation de Ridge et de régression Lasso sont les meilleurs modèles de statistiques prédictives que l'on a trouvé. En effet, ils ont les meilleures AUC et les meilleures accuracy. De plus, le modèle de régression Lasso dépend de moins de variables que le modèle de régression Ridge, ce qui peut être un avantage. 

Le modèle de SVM avec noyau RBF est le meilleur modèle en terme d'AUC, mais il n'est pas le meilleur en terme d'accuracy.

Le modèle de régression logistique sans pénalisation et le modèle CART sont les moins bons modèles que l'on a testés Cela peut s'expliquer par une mauvaise calibration pour le modèle de régression logistique. 

# Conclusion

Le data-set est exploitable en machine learning, et nous avons pu obtenir des modèles de statistiques prédictives qui ont de bonnes performances. Les modèles de régression logistique avec pénalisation de Ridge et de régression Lasso sont les meilleurs modèles que l'on a trouvé. Ces modèles pourraient permettre de prédire le diagnostic de patientes atteintes de cancer du sein, dans une certaine mesure.
